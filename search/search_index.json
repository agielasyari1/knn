{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KNN or K-Nearest Neighbours Apa Itu KNN (K-Nearest Neighbour) ?? Algoritma K-Nearest Neighbor (KNN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data . Kelebihan dan Kekurangan KNN (K-Nearest Neighbor) Kelebihan KNN (K-Nearest Neighbour) Sangat nonlinear. Mudah dipahami dan diimplementasikan. Kekurangan KNN (K-Nearest Neighbour) Perlu menunjukkan parameter K (jumlah tetangga terdekat). Tidak menangani nilai hilang ( missing value ) secara implisit. Sensitif terhadap data pencilan ( outlier ). Rentan terhadap variabel yang non-informatif. Rentan terhadap dimensionalitas yang tinggi. Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari setiap sampel uji pada keseluruhan sampel latih. Study Kasus Data yang digunakan pada kali ini adalah data yang diperoleh dari www.kaggle.com yaitu Social_Network_Ads.csv. Step 1, Import package dengan menggunakan python 3.7 library yang diperlukan : # -*- coding: utf-8 -*- \"\"\" Created on Mon May 6 20:06:25 2019 @author: agielasyari1 \"\"\" #imporing libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd import time Step 2, Feature scalling Wikipedia- \u201cSince the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\u201d \u200b Jadi karena klasifikasi KNN dihitung berdasarkan jarak Euclidean antar dua titik. Jika salah satu variabel memiliki jarak yang dengan rentang yang jauh(nilai yg jauh lebih tinggi),maka nilai tersebut harus disesuaikan. Jadi jarak antara semua variabel harus dinormalisasikan agar jarak akhir yang didapatkan proposional. #feature scaling class FeatureScaling: def __init__(self,X,y): self.X=X.copy() if y.ndim==1: y=np.reshape(y,(y.shape[0],1)) self.y=y.copy() self.minMax_X={} self.minMax_y={} def fit_transform_X(self): num_of_features=self.X.shape[1] for i in range(num_of_features): feature=self.X[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_X[i]=np.array([Mean,Min,Max]) self.X[:,i]=feature return self.X.copy() def fit_transform_Y(self): num_of_features=self.y.shape[1] for i in range(num_of_features): feature=self.y[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_y[i]=np.array([Mean,Min,Max]) self.y[:,i]=feature return np.reshape(self.y,self.y.shape[0]) def inverse_transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_X[i][1] Max=self.minMax_X[i][2] feature=feature*(Max-Min)+Mean X_transformed[:,i]=feature return X_transformed def inverse_transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=feature*(Max-Min)+Mean y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) X_transformed[:,i]=feature return X_transformed def transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def returnX(self): return self.X def returnY(self): return self.y Step 3 Train \u200b Saya menerapkan kelas KNN dengan fungsi standar 'cocok' untuk pelatihan dan 'prediksi' untuk memprediksi data uji. KNN menggunakan lazy algoritm yang berarti semua perhitungan ditangguhkan hingga prediksi. Dalam metode fit, saya hanya menetapkan data pelatihan ke variabel kelas - xtrain dan ytrain. Tidak diperlukan perhitungan. \u200b Saat saya mengulangi setiap baris pelatihan untuk mendapatkan skor kesamaan, saya menggunakan document_similarity fungsi kustom yang menerima dua teks dan mengembalikan skor kesamaan di antara mereka (0 & 1). Skor kesamaan yang lebih tinggi menunjukkan lebih banyak kesamaan di antara mereka. import numpy as np class KNN: def __init__(self,X_train,Y_train,K): self.X_train=X_train self.Y_train=Y_train self.K=K def predict(self,X): y_pred=np.array([]) for each in X: ed=np.sum((each-self.X_train)**2,axis=1) y_ed=np.concatenate((self.Y_train.reshape(self.Y_train.shape[0],1),ed.reshape(ed.shape[0],1)),axis=1) y_ed=y_ed[y_ed[:,1].argsort()] K_neighbours=y_ed[0:self.K] (values,counts) = np.unique(K_neighbours[:,0].astype(int),return_counts=True) y_pred=np.append(y_pred,values[np.argmax(counts)]) return y_pred Step 4 , Read data #reading dataset Data=pd.read_csv('Social_Network_Ads.csv') print(Data.head(10)) Data.describe() Output User ID Gender Age EstimatedSalary Purchased 0 15624510 Male 19 19000 0 1 15810944 Male 35 20000 0 2 15668575 Female 26 43000 0 3 15603246 Female 27 57000 0 4 15804002 Male 19 76000 0 5 15728773 Male 27 58000 0 6 15598044 Female 27 84000 0 7 15694829 Female 32 150000 1 8 15600575 Male 25 33000 0 9 15727311 Female 35 65000 0","title":"KNN or K-Nearest Neighbours"},{"location":"#knn-or-k-nearest-neighbours","text":"","title":"KNN or K-Nearest Neighbours"},{"location":"#apa-itu-knn-k-nearest-neighbour","text":"Algoritma K-Nearest Neighbor (KNN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN. Tujuan dari algoritma ini adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data .","title":"Apa Itu KNN (K-Nearest Neighbour) ??"},{"location":"#kelebihan-dan-kekurangan-knn-k-nearest-neighbor","text":"Kelebihan KNN (K-Nearest Neighbour) Sangat nonlinear. Mudah dipahami dan diimplementasikan. Kekurangan KNN (K-Nearest Neighbour) Perlu menunjukkan parameter K (jumlah tetangga terdekat). Tidak menangani nilai hilang ( missing value ) secara implisit. Sensitif terhadap data pencilan ( outlier ). Rentan terhadap variabel yang non-informatif. Rentan terhadap dimensionalitas yang tinggi. Biaya komputasi cukup tinggi karena diperlukan perhitungan jarak dari setiap sampel uji pada keseluruhan sampel latih.","title":"Kelebihan dan Kekurangan KNN (K-Nearest Neighbor)"},{"location":"#study-kasus","text":"Data yang digunakan pada kali ini adalah data yang diperoleh dari www.kaggle.com yaitu Social_Network_Ads.csv.","title":"Study Kasus"},{"location":"#step-1-import-package","text":"dengan menggunakan python 3.7 library yang diperlukan : # -*- coding: utf-8 -*- \"\"\" Created on Mon May 6 20:06:25 2019 @author: agielasyari1 \"\"\" #imporing libraries import numpy as np import matplotlib.pyplot as plt import pandas as pd import time","title":"Step 1, Import package"},{"location":"#step-2-feature-scalling","text":"Wikipedia- \u201cSince the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\u201d \u200b Jadi karena klasifikasi KNN dihitung berdasarkan jarak Euclidean antar dua titik. Jika salah satu variabel memiliki jarak yang dengan rentang yang jauh(nilai yg jauh lebih tinggi),maka nilai tersebut harus disesuaikan. Jadi jarak antara semua variabel harus dinormalisasikan agar jarak akhir yang didapatkan proposional. #feature scaling class FeatureScaling: def __init__(self,X,y): self.X=X.copy() if y.ndim==1: y=np.reshape(y,(y.shape[0],1)) self.y=y.copy() self.minMax_X={} self.minMax_y={} def fit_transform_X(self): num_of_features=self.X.shape[1] for i in range(num_of_features): feature=self.X[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_X[i]=np.array([Mean,Min,Max]) self.X[:,i]=feature return self.X.copy() def fit_transform_Y(self): num_of_features=self.y.shape[1] for i in range(num_of_features): feature=self.y[:,i] Mean=np.mean(feature) Min=np.min(feature) Max=np.max(feature) feature=(feature-Mean)/(Max-Min) self.minMax_y[i]=np.array([Mean,Min,Max]) self.y[:,i]=feature return np.reshape(self.y,self.y.shape[0]) def inverse_transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_X[i][1] Max=self.minMax_X[i][2] feature=feature*(Max-Min)+Mean X_transformed[:,i]=feature return X_transformed def inverse_transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=feature*(Max-Min)+Mean y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def transform_X(self,X): X_transformed=X.copy() num_of_features=X_transformed.shape[1] for i in range(num_of_features): feature=X_transformed[:,i] Mean=self.minMax_X[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) X_transformed[:,i]=feature return X_transformed def transform_Y(self,y): y_transformed=y.copy() if y_transformed.ndim==1: y_transformed=np.reshape(y_transformed,(y_transformed.shape[0],1)) num_of_features=y_transformed.shape[1] for i in range(num_of_features): feature=y_transformed[:,i] Mean=self.minMax_y[i][0] Min=self.minMax_y[i][1] Max=self.minMax_y[i][2] feature=(feature-Mean)/(Max-Min) y_transformed[:,i]=feature return np.reshape(y_transformed,y_transformed.shape[0]) def returnX(self): return self.X def returnY(self): return self.y","title":"Step 2, Feature scalling"},{"location":"#step-3-train","text":"\u200b Saya menerapkan kelas KNN dengan fungsi standar 'cocok' untuk pelatihan dan 'prediksi' untuk memprediksi data uji. KNN menggunakan lazy algoritm yang berarti semua perhitungan ditangguhkan hingga prediksi. Dalam metode fit, saya hanya menetapkan data pelatihan ke variabel kelas - xtrain dan ytrain. Tidak diperlukan perhitungan. \u200b Saat saya mengulangi setiap baris pelatihan untuk mendapatkan skor kesamaan, saya menggunakan document_similarity fungsi kustom yang menerima dua teks dan mengembalikan skor kesamaan di antara mereka (0 & 1). Skor kesamaan yang lebih tinggi menunjukkan lebih banyak kesamaan di antara mereka. import numpy as np class KNN: def __init__(self,X_train,Y_train,K): self.X_train=X_train self.Y_train=Y_train self.K=K def predict(self,X): y_pred=np.array([]) for each in X: ed=np.sum((each-self.X_train)**2,axis=1) y_ed=np.concatenate((self.Y_train.reshape(self.Y_train.shape[0],1),ed.reshape(ed.shape[0],1)),axis=1) y_ed=y_ed[y_ed[:,1].argsort()] K_neighbours=y_ed[0:self.K] (values,counts) = np.unique(K_neighbours[:,0].astype(int),return_counts=True) y_pred=np.append(y_pred,values[np.argmax(counts)]) return y_pred","title":"Step 3 Train"},{"location":"#step-4-read-data","text":"#reading dataset Data=pd.read_csv('Social_Network_Ads.csv') print(Data.head(10)) Data.describe()","title":"Step 4 , Read data"},{"location":"#output","text":"User ID Gender Age EstimatedSalary Purchased 0 15624510 Male 19 19000 0 1 15810944 Male 35 20000 0 2 15668575 Female 26 43000 0 3 15603246 Female 27 57000 0 4 15804002 Male 19 76000 0 5 15728773 Male 27 58000 0 6 15598044 Female 27 84000 0 7 15694829 Female 32 150000 1 8 15600575 Male 25 33000 0 9 15727311 Female 35 65000 0","title":"Output"}]}